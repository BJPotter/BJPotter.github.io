---
title: Kettel
tag: 
- ETL
---

# 简介

>Kettle，由Pentaho公司开发和维护,是一款开源的ETL(Extract、Transform、Load)工具，所以也称为Pentaho Data Integration(PDI)。
>
>Kettle主要用于数据的抽取、转换和加载，为用户提供了一系列丰富的数据整合功能，主要目标是将原始数据转化为有价值、有意义的信息。

# Kettle的安装与配置

## JAVA环境配置

>Kettle是基于JAVA开发的，如果运行需要配置JAVA_HOME环境变量（或者配置PENTAHO_JAVA_HOME），建议安装版本JDK1.8版本及以上

```shell
# 系统变量
变量名：`JAVA_HOME` 变量值：`D:\jdk1.8.0_291`
# PATH变量
变量值： `%JAVA_HOME%\bin`
# 验证
cmd -> `java -version`
```

## Kettle安装

>下载kettle,解压到本地,双击Spoon.bat 就能启动kettle。 



# Kettle的四大组件

## Spooner（转换工具）

>Spooner是Kettle的图形化设计工具，它开源并且易于使用。在这里，你可以创建和编辑PDI的转换和作业。通过拖拽图形组件，可以很方便地进行数据连接、转换设计以及调试排错。它的可视化界面使得复杂的数据处理流程变得清晰和直观。

## Kitchen（执行工具）

>Kitchen是在命令行环境下，用来执行kettle作业的工具。你可以不依赖图形化界面，在脚本或者计划任务中直接使用Kitchen来执行定义好的作业，为自动化数据处理提供了可能。

## Pan（执行工具）

>类似于Kitchen，Pan也是一个命令行工具，区别在于它用来执行的是单一的数据转换。当你希望在命令行中独立运行和调度数据转换时，Pan工具就是你的选择。

## Carte（集群服务）

>Carte是Kettle提供的轻量级的Web容器，它可以把构建在Spoon上的ETL转换和作业发布到服务器上执行，支持远程的ETL任务的运行和监控。更重要的是，Carte支持集群配置，能在多台服务器间分发转换运算，满足大规模数据处理的需求

# 数据转换与集成

## 数据转换

1. 数据清洗：这是数据预处理的第一步，主要包括对空值、重复值、异常值的处理，以及数据的规范化、验证和去噪声处理等操作。

2. 数据标准化：转换原始数据，使其服从一定的数据标准模式或数据规范，如单位转换、数据归一化等。

3. 数据构建：根据业务需求，从原始数据中获取新的特征数据，并进一步创建数据模型。

4. 数据聚合：对数据进行汇总抽象，如进行总计、平均、最大值、最小值等统计操作。

## 数据集成

1. 数据同步：即将分散在不同源系统中的数据，按照特定的规则同步更新到一个或多个目标系统中。

2. 数据联合：将来自不同数据源的关联数据结合在一起，提供统一的数据视图，实现对同一业务主题的全面分析。

3. 数据仓库建设：利用数据集成技术，将企业内部的业务数据进行提炼、转换和加载，构建起满足多维度分析需求的数据仓库。

# 操作案例

## 转换和作业

转换：转换包括一个或多个步骤，步骤之间通过跳（hop）来连接。跳定义了一个单向通道，允许数据从一个步骤流向另一个步骤。在Kettle中，数据的单位是行，数据流就是数据行从一个步骤到另一个步骤的移动。

作业：作业包含了一系列转换，如果要定时执行一些列步骤，需要用到作业。

## 输入和输出

### CSV输入

新建 --> 转换 --> 输入 --> CSV文件输入 --> 文件名（指定文件位置）--> 获取字段（改成String）--> 点击预览

### Excel输入

新建 --> 转换 --> 输入 --> Excel input文件输入 --> 表引擎（Excel 2007 XLSX (Apache POI Streaming)）-->浏览（指定文件位置）--> 增加-->获取字段（改成String）--> 点击预览

### 文本文件输出

新建 --> 转换 --> 输出 --> 文本文件输出 --> 浏览（指定输出位置）--> 扩展名 --> 分隔符

### Excel输出

新建 --> 转换 -->输出 --> Excel output文件输出 --> 浏览（指定输出文件位置）

## 脚本执行

### SQL脚本

### SQL表ToExcel











# 高级使用

## 参数和变量的使用

在Kettle的工作中，我们经常需要处理不同数据源或者不同时间的数据，这时候参数和变量的使用就显得非常有价值。我们可以利用Kettle的参数支持动态设置数据库连接或者数据文件的路径，变量则可以让我们自由地控制转换或作业的流程。

## 数据服务发布

Kettle的Carte组件支持将您的转换发布为数据服务供其他系统调用，这是一种非常方便的数据交互方式。通过发布数据服务，您可以将Kettle转换或作业封装为一个RESTful风格的Web服务，称为“数据服务”。其他系统可以通过HTTP的GET或POST方法调用此数据服务，提供需求参数并获得数据结果。

## 大数据处理

Kettle早在4.4版本就开始支持对大数据的处理，包括Hadoop、Hive、Hbase、Pig和Spark等大数据技术，在5.x以后的版本中，更是大力推广和完善对于大数据处理的支持，用户可以直接在Kettle转换中使用各种大数据组件，极大地方便了ETL开发人员在大数据处理中的工作。

## 插件开发

Kettle支持用户自定义开发插件，如果原生的Kettle功能还不能满足你的需求，你可以考虑开发自己的插件扩展Kettle的功能，实现一些特定的处理逻辑。

## 集群配置

对于大规模数据处理的需求，我们可以使用Carte组件配置Kettle集群，通过在多个节点上分发运行转换，实现并行处理，增加执行效率。

